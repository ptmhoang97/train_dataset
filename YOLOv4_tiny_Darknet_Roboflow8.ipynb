{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YOLOv4-tiny-Darknet-Roboflow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ptmhoang97/train_with_TensorFlow2.0/blob/main/YOLOv4_tiny_Darknet_Roboflow8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive._mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "iiOKIEXDyOa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSyF2MKS1GBF"
      },
      "source": [
        "# Choose dataset to unzip\n",
        "dataset_name = \"coco2017_train_truck_YOLOv4\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDTvGt2zt7cm"
      },
      "source": [
        "# Configuring cuDNN on Colab for YOLOv4\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-bguKWgtxSx"
      },
      "source": [
        "# CUDA: Let's check that Nvidia CUDA drivers are already pre-installed and which version is it.\n",
        "!/usr/local/cuda/bin/nvcc --version\n",
        "# We need to install the correct cuDNN according to this output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6BRAVo182G5"
      },
      "source": [
        "#take a look at the kind of GPU we have\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fb5mFF-RyBAL"
      },
      "source": [
        "# This cell ensures you have the correct architecture for your respective GPU\n",
        "# If you command is not found, look through these GPUs, find the respective\n",
        "# GPU and add them to the archTypes dictionary\n",
        "\n",
        "# Tesla V100\n",
        "# ARCH= -gencode arch=compute_70,code=[sm_70,compute_70]\n",
        "\n",
        "# Tesla K80 \n",
        "# ARCH= -gencode arch=compute_37,code=sm_37\n",
        "\n",
        "# GeForce RTX 2080 Ti, RTX 2080, RTX 2070, Quadro RTX 8000, Quadro RTX 6000, Quadro RTX 5000, Tesla T4, XNOR Tensor Cores\n",
        "# ARCH= -gencode arch=compute_75,code=[sm_75,compute_75]\n",
        "\n",
        "# Jetson XAVIER\n",
        "# ARCH= -gencode arch=compute_72,code=[sm_72,compute_72]\n",
        "\n",
        "# GTX 1080, GTX 1070, GTX 1060, GTX 1050, GTX 1030, Titan Xp, Tesla P40, Tesla P4\n",
        "# ARCH= -gencode arch=compute_61,code=sm_61\n",
        "\n",
        "# GP100/Tesla P100 - DGX-1\n",
        "# ARCH= -gencode arch=compute_60,code=sm_60\n",
        "\n",
        "# For Jetson TX1, Tegra X1, DRIVE CX, DRIVE PX - uncomment:\n",
        "# ARCH= -gencode arch=compute_53,code=[sm_53,compute_53]\n",
        "\n",
        "# For Jetson Tx2 or Drive-PX2 uncomment:\n",
        "# ARCH= -gencode arch=compute_62,code=[sm_62,compute_62]\n",
        "import os\n",
        "os.environ['GPU_TYPE'] = str(os.popen('nvidia-smi --query-gpu=name --format=csv,noheader').read())\n",
        "\n",
        "def getGPUArch(argument):\n",
        "  try:\n",
        "    argument = argument.strip()\n",
        "    # All Colab GPUs\n",
        "    archTypes = {\n",
        "        \"Tesla V100-SXM2-16GB\": \"-gencode arch=compute_70,code=[sm_70,compute_70]\",\n",
        "        \"Tesla K80\": \"-gencode arch=compute_37,code=sm_37\",\n",
        "        \"Tesla T4\": \"-gencode arch=compute_75,code=[sm_75,compute_75]\",\n",
        "        \"Tesla P40\": \"-gencode arch=compute_61,code=sm_61\",\n",
        "        \"Tesla P4\": \"-gencode arch=compute_61,code=sm_61\",\n",
        "        \"Tesla P100-PCIE-16GB\": \"-gencode arch=compute_60,code=sm_60\"\n",
        "\n",
        "      }\n",
        "    return archTypes[argument]\n",
        "  except KeyError:\n",
        "    return \"GPU must be added to GPU Commands\"\n",
        "os.environ['ARCH_VALUE'] = getGPUArch(os.environ['GPU_TYPE'])\n",
        "\n",
        "print(\"GPU Type: \" + os.environ['GPU_TYPE'])\n",
        "print(\"ARCH Value: \" + os.environ['ARCH_VALUE'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16pvdFMa1FEe"
      },
      "source": [
        "# Installing Darknet for YOLOv4 on Colab\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9uY-38P93oz"
      },
      "source": [
        "%cd /content/\n",
        "%rm -rf darknet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQEktcfj9y9O"
      },
      "source": [
        "#we clone the fork of darknet maintained by roboflow\n",
        "#small changes have been made to configure darknet for training\n",
        "!git clone https://github.com/roboflow-ai/darknet.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyMBDkaL-Aep"
      },
      "source": [
        "#install environment from the Makefile\n",
        "%cd /content/darknet/\n",
        "# compute_37, sm_37 for Tesla K80\n",
        "# compute_75, sm_75 for Tesla T4\n",
        "# !sed -i 's/ARCH= -gencode arch=compute_60,code=sm_60/ARCH= -gencode arch=compute_75,code=sm_75/g' Makefile\n",
        "\n",
        "#install environment from the Makefile\n",
        "#note if you are on Colab Pro this works on a P100 GPU\n",
        "#if you are on Colab free, you may need to change the Makefile for the K80 GPU\n",
        "#this goes for any GPU, you need to change the Makefile to inform darknet which GPU you are running on.\n",
        "!sed -i 's/OPENCV=0/OPENCV=1/g' Makefile\n",
        "!sed -i 's/GPU=0/GPU=1/g' Makefile\n",
        "!sed -i 's/CUDNN=0/CUDNN=1/g' Makefile\n",
        "!sed -i \"s/ARCH= -gencode arch=compute_60,code=sm_60/ARCH= ${ARCH_VALUE}/g\" Makefile\n",
        "!make"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGPDEjfAALrQ"
      },
      "source": [
        "#download the newly released yolov4-tiny weights\n",
        "%cd /content/darknet\n",
        "!wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v4_pre/yolov4-tiny.weights\n",
        "!wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v4_pre/yolov4-tiny.conv.29"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWOiKj37l4wW"
      },
      "source": [
        "# Set up Custom Dataset for YOLOv4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbniFj-eSimL"
      },
      "source": [
        "We'll use Roboflow to convert our dataset from any format to the YOLO Darknet format. \n",
        "\n",
        "1. To do so, create a free [Roboflow account](https://app.roboflow.ai).\n",
        "2. Upload your images and their annotations (in any format: VOC XML, COCO JSON, TensorFlow CSV, etc).\n",
        "3. Apply preprocessing and augmentation steps you may like. We recommend at least `auto-orient` and a `resize` to 416x416. Generate your dataset.\n",
        "4. Export your dataset in the **YOLO Darknet format**.\n",
        "5. Copy your download link, and paste it below.\n",
        "\n",
        "See our [blog post](https://blog.roboflow.ai/training-yolov4-on-a-custom-dataset/) for greater detail."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCpKg0wYOeGR"
      },
      "source": [
        "dataset_path = \"/content/gdrive/MyDrive/_dataset/{0}.zip\".format(dataset_name)\n",
        "!unzip {dataset_path} -d \"/content/darknet\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_path = \"{0}/train\".format(dataset_name)\n",
        "test_dataset_path = \"{0}/test\".format(dataset_name)"
      ],
      "metadata": {
        "id": "yDoRmrEnKGx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# _darknet.labels is similar between train and test, need only 1 file\n",
        "train_dataset_darknet_label_path = \"{0}/_darknet.labels\".format(train_dataset_path)"
      ],
      "metadata": {
        "id": "me49tsYAKlbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# classes.txt from labelimg tool\n",
        "train_dataset_class_txt_path = \"{0}/classes.txt\".format(train_dataset_path)\n",
        "test_dataset_class_txt_path = \"{0}/classes.txt\".format(test_dataset_path)"
      ],
      "metadata": {
        "id": "pgD73TyOLUyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_jpg_path = \"{0}/*.jpg\".format(train_dataset_path)\n",
        "test_dataset_jpg_path = \"{0}/*.jpg\".format(test_dataset_path)"
      ],
      "metadata": {
        "id": "I9jTv_FEL0p8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_anno_path = \"{0}/*.txt\".format(train_dataset_path)\n",
        "test_dataset_anno_path = \"{0}/*.txt\".format(test_dataset_path)"
      ],
      "metadata": {
        "id": "e9_8dAx9MFFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cp {train_dataset_darknet_label_path} data/obj.names"
      ],
      "metadata": {
        "id": "JPzTIyNnK8OU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFP-MMdtiSjB"
      },
      "source": [
        "%mkdir data/obj"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%rm {train_dataset_class_txt_path}\n",
        "%rm {test_dataset_class_txt_path}"
      ],
      "metadata": {
        "id": "qTmdS2Nb0Kdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nNMSMviiTsB"
      },
      "source": [
        "%cp {train_dataset_jpg_path} data/obj/\n",
        "%cp {test_dataset_jpg_path} data/obj/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tq1pzsT8iUkh"
      },
      "source": [
        "%cp {train_dataset_anno_path} data/obj/\n",
        "%cp {test_dataset_anno_path} data/obj/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def file_len(fname):\n",
        "  with open(fname) as f:\n",
        "    for i, l in enumerate(f):\n",
        "      pass\n",
        "  return i + 1\n",
        "num_classes = file_len(train_dataset_darknet_label_path)\n",
        "\n",
        "print(num_classes)"
      ],
      "metadata": {
        "id": "3EE7iAITNUtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('data/obj.data', 'w') as out:\n",
        "    out.write('classes = ' + str(num_classes) + '\\n')\n",
        "    out.write('train = data/train.txt\\n')\n",
        "    out.write('test = data/test.txt\\n')\n",
        "    out.write('names = data/obj.names\\n')\n",
        "    #out.write('backup = backup/')\n",
        "    out.write('backup = /content/gdrive/MyDrive/_trainYOLO/backup/')"
      ],
      "metadata": {
        "id": "nz1sb6ikN3rV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#write train file (just the image list)\n",
        "import os\n",
        "\n",
        "with open('data/train.txt', 'w') as out:\n",
        "  for img in [f for f in os.listdir(train_dataset_path) if f.endswith('jpg')]:\n",
        "    out.write('data/obj/' + img + '\\n')"
      ],
      "metadata": {
        "id": "JO9pCUDeOBQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiCILEbs1NII"
      },
      "source": [
        "#write the test file (just the image list)\n",
        "import os\n",
        "\n",
        "with open('data/test.txt', 'w') as out:\n",
        "  for img in [f for f in os.listdir(test_dataset_path) if f.endswith('jpg')]:\n",
        "    out.write('data/obj/' + img + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HtRqO3QvjkP"
      },
      "source": [
        "# Write Custom Training Config for YOLOv4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_WJcqHhpeVr"
      },
      "source": [
        "import os\n",
        "\n",
        "#we build config dynamically based on number of classes\n",
        "#we build iteratively from base config files. This is the same file shape as cfg/yolo-obj.cfg\n",
        "\n",
        "#Instructions from the darknet repo\n",
        "#change line max_batches to (classes*2000 but not less than number of training images, and not less than 6000), f.e. max_batches=6000 if you train for 3 classes\n",
        "#change line steps to 80% and 90% of max_batches, f.e. steps=4800,5400\n",
        "max_batches = num_classes*2000\n",
        "\n",
        "num_pics_train = (len([name for name in os.listdir(train_dataset_path.format()) if os.path.isfile(os.path.join(train_dataset_path, name))]) - 1)/2\n",
        "num_pics_test  = (len([name for name in os.listdir(test_dataset_path.format()) if os.path.isfile(os.path.join(test_dataset_path, name))]) - 1)/2\n",
        "num_pics_total = num_pics_train + num_pics_test\n",
        "\n",
        "standard_batches = max(6000, num_pics_total)\n",
        "\n",
        "if max_batches < standard_batches:\n",
        "   max_batches = standard_batches\n",
        "else:\n",
        "  pass\n",
        "\n",
        "print(\"num_pics_train: \", num_pics_train)\n",
        "print(\"num_pics_test: \", num_pics_test)\n",
        "print(\"num_pics_total: \", num_pics_total)\n",
        "print(\"standard_batches: \", standard_batches)\n",
        "print(\"max_batches: \", max_batches)\n",
        "\n",
        "steps1 = .8 * max_batches\n",
        "steps2 = .9 * max_batches\n",
        "steps_str = str(steps1)+','+str(steps2)\n",
        "num_filters = (num_classes + 5) * 3\n",
        "\n",
        "print(\"writing config for a custom YOLOv4 detector detecting number of classes: \" + str(num_classes))\n",
        "\n",
        "if os.path.exists('./cfg/custom-yolov4-tiny-detector.cfg'): os.remove('./cfg/custom-yolov4-tiny-detector.cfg')\n",
        "\n",
        "\n",
        "#customize iPython writefile so we can write variables\n",
        "from IPython.core.magic import register_line_cell_magic\n",
        "\n",
        "@register_line_cell_magic\n",
        "def writetemplate(line, cell):\n",
        "    with open(line, 'w') as f:\n",
        "        f.write(cell.format(**globals()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03VuD4NHnxFx"
      },
      "source": [
        "%%writetemplate ./cfg/custom-yolov4-tiny-detector.cfg\n",
        "[net]\n",
        "# Testing\n",
        "#batch=1\n",
        "#subdivisions=1\n",
        "# Training\n",
        "batch=64\n",
        "subdivisions=24\n",
        "width=416\n",
        "height=416\n",
        "channels=3\n",
        "momentum=0.9\n",
        "decay=0.0005\n",
        "angle=0\n",
        "saturation = 1.5\n",
        "exposure = 1.5\n",
        "hue=.1\n",
        "\n",
        "learning_rate=0.00261\n",
        "burn_in=1000\n",
        "max_batches = {max_batches}\n",
        "policy=steps\n",
        "steps={steps_str}\n",
        "scales=.1,.1\n",
        "\n",
        "[convolutional]\n",
        "batch_normalize=1\n",
        "filters=32\n",
        "size=3\n",
        "stride=2\n",
        "pad=1\n",
        "activation=leaky\n",
        "\n",
        "[convolutional]\n",
        "batch_normalize=1\n",
        "filters=64\n",
        "size=3\n",
        "stride=2\n",
        "pad=1\n",
        "activation=leaky\n",
        "\n",
        "[convolutional]\n",
        "batch_normalize=1\n",
        "filters=64\n",
        "size=3\n",
        "stride=1\n",
        "pad=1\n",
        "activation=leaky\n",
        "\n",
        "[route]\n",
        "layers=-1\n",
        "groups=2\n",
        "group_id=1\n",
        "\n",
        "[convolutional]\n",
        "batch_normalize=1\n",
        "filters=32\n",
        "size=3\n",
        "stride=1\n",
        "pad=1\n",
        "activation=leaky\n",
        "\n",
        "[convolutional]\n",
        "batch_normalize=1\n",
        "filters=32\n",
        "size=3\n",
        "stride=1\n",
        "pad=1\n",
        "activation=leaky\n",
        "\n",
        "[route]\n",
        "layers = -1,-2\n",
        "\n",
        "[convolutional]\n",
        "batch_normalize=1\n",
        "filters=64\n",
        "size=1\n",
        "stride=1\n",
        "pad=1\n",
        "activation=leaky\n",
        "\n",
        "[route]\n",
        "layers = -6,-1\n",
        "\n",
        "[maxpool]\n",
        "size=2\n",
        "stride=2\n",
        "\n",
        "[convolutional]\n",
        "batch_normalize=1\n",
        "filters=128\n",
        "size=3\n",
        "stride=1\n",
        "pad=1\n",
        "activation=leaky\n",
        "\n",
        "[route]\n",
        "layers=-1\n",
        "groups=2\n",
        "group_id=1\n",
        "\n",
        "[convolutional]\n",
        "batch_normalize=1\n",
        "filters=64\n",
        "size=3\n",
        "stride=1\n",
        "pad=1\n",
        "activation=leaky\n",
        "\n",
        "[convolutional]\n",
        "batch_normalize=1\n",
        "filters=64\n",
        "size=3\n",
        "stride=1\n",
        "pad=1\n",
        "activation=leaky\n",
        "\n",
        "[route]\n",
        "layers = -1,-2\n",
        "\n",
        "[convolutional]\n",
        "batch_normalize=1\n",
        "filters=128\n",
        "size=1\n",
        "stride=1\n",
        "pad=1\n",
        "activation=leaky\n",
        "\n",
        "[route]\n",
        "layers = -6,-1\n",
        "\n",
        "[maxpool]\n",
        "size=2\n",
        "stride=2\n",
        "\n",
        "[convolutional]\n",
        "batch_normalize=1\n",
        "filters=256\n",
        "size=3\n",
        "stride=1\n",
        "pad=1\n",
        "activation=leaky\n",
        "\n",
        "[route]\n",
        "layers=-1\n",
        "groups=2\n",
        "group_id=1\n",
        "\n",
        "[convolutional]\n",
        "batch_normalize=1\n",
        "filters=128\n",
        "size=3\n",
        "stride=1\n",
        "pad=1\n",
        "activation=leaky\n",
        "\n",
        "[convolutional]\n",
        "batch_normalize=1\n",
        "filters=128\n",
        "size=3\n",
        "stride=1\n",
        "pad=1\n",
        "activation=leaky\n",
        "\n",
        "[route]\n",
        "layers = -1,-2\n",
        "\n",
        "[convolutional]\n",
        "batch_normalize=1\n",
        "filters=256\n",
        "size=1\n",
        "stride=1\n",
        "pad=1\n",
        "activation=leaky\n",
        "\n",
        "[route]\n",
        "layers = -6,-1\n",
        "\n",
        "[maxpool]\n",
        "size=2\n",
        "stride=2\n",
        "\n",
        "[convolutional]\n",
        "batch_normalize=1\n",
        "filters=512\n",
        "size=3\n",
        "stride=1\n",
        "pad=1\n",
        "activation=leaky\n",
        "\n",
        "##################################\n",
        "\n",
        "[convolutional]\n",
        "batch_normalize=1\n",
        "filters=256\n",
        "size=1\n",
        "stride=1\n",
        "pad=1\n",
        "activation=leaky\n",
        "\n",
        "[convolutional]\n",
        "batch_normalize=1\n",
        "filters=512\n",
        "size=3\n",
        "stride=1\n",
        "pad=1\n",
        "activation=leaky\n",
        "\n",
        "[convolutional]\n",
        "size=1\n",
        "stride=1\n",
        "pad=1\n",
        "filters={num_filters}\n",
        "activation=linear\n",
        "\n",
        "\n",
        "\n",
        "[yolo]\n",
        "mask = 3,4,5\n",
        "anchors = 10,14,  23,27,  37,58,  81,82,  135,169,  344,319\n",
        "classes={num_classes}\n",
        "num=6\n",
        "jitter=.3\n",
        "scale_x_y = 1.05\n",
        "cls_normalizer=1.0\n",
        "iou_normalizer=0.07\n",
        "iou_loss=ciou\n",
        "ignore_thresh = .7\n",
        "truth_thresh = 1\n",
        "random=0\n",
        "nms_kind=greedynms\n",
        "beta_nms=0.6\n",
        "\n",
        "[route]\n",
        "layers = -4\n",
        "\n",
        "[convolutional]\n",
        "batch_normalize=1\n",
        "filters=128\n",
        "size=1\n",
        "stride=1\n",
        "pad=1\n",
        "activation=leaky\n",
        "\n",
        "[upsample]\n",
        "stride=2\n",
        "\n",
        "[route]\n",
        "layers = -1, 23\n",
        "\n",
        "[convolutional]\n",
        "batch_normalize=1\n",
        "filters=256\n",
        "size=3\n",
        "stride=1\n",
        "pad=1\n",
        "activation=leaky\n",
        "\n",
        "[convolutional]\n",
        "size=1\n",
        "stride=1\n",
        "pad=1\n",
        "filters={num_filters}\n",
        "activation=linear\n",
        "\n",
        "[yolo]\n",
        "mask = 1,2,3\n",
        "anchors = 10,14,  23,27,  37,58,  81,82,  135,169,  344,319\n",
        "classes={num_classes}\n",
        "num=6\n",
        "jitter=.3\n",
        "scale_x_y = 1.05\n",
        "cls_normalizer=1.0\n",
        "iou_normalizer=0.07\n",
        "iou_loss=ciou\n",
        "ignore_thresh = .7\n",
        "truth_thresh = 1\n",
        "random=0\n",
        "nms_kind=greedynms\n",
        "beta_nms=0.6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2LAciMh4Cut"
      },
      "source": [
        "#here is the file that was just written. \n",
        "#you may consider adjusting certain things\n",
        "\n",
        "#like the number of subdivisions 64 runs faster but Colab GPU may not be big enough\n",
        "#if Colab GPU memory is too small, you will need to adjust subdivisions to 16\n",
        "%cat cfg/custom-yolov4-tiny-detector.cfg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNc8YCHGig_A"
      },
      "source": [
        "# Re-train from last weight"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBpnBpK7ikMI"
      },
      "source": [
        "last_weight = \"/content/gdrive/MyDrive/_trainYOLO/backup/custom-yolov4-tiny-detector_last.weights\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zVnRyRAii7B"
      },
      "source": [
        "!./darknet detector train data/obj.data cfg/custom-yolov4-tiny-detector.cfg {last_weight} -dont_show -map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWrG9EGamSpH"
      },
      "source": [
        "# Train from scratch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        " \n",
        "dir = '/content/gdrive/MyDrive/_trainYOLO/backup'\n",
        "for f in os.listdir(dir):\n",
        "    os.remove(os.path.join(dir, f))"
      ],
      "metadata": {
        "id": "HOopjZfV7aBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"/content/darknet/cfg/custom-yolov4-tiny-detector.cfg\" \"/content/gdrive/MyDrive/_trainYOLO/backup\""
      ],
      "metadata": {
        "id": "BC02N8437bit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6miYFbvExqMd"
      },
      "source": [
        "!./darknet detector train data/obj.data cfg/custom-yolov4-tiny-detector.cfg yolov4-tiny.conv.29 -dont_show -map\n",
        "#If you get CUDA out of memory adjust subdivisions above!\n",
        "#adjust max batches down for shorter training above"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}